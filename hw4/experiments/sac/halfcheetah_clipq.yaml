# Use clipped double-Q learning (from TD3)
num_critic_networks: 2
target_critic_backup_type: min

base_config: sac

batch_size: 128

discount: 0.99
use_soft_target_update: true
soft_target_update_rate: 0.005

actor_gradient_type: reparametrize
num_critic_updates: 1

use_entropy_bonus: true
temperature: 0.05

num_agent_train_steps_per_iter: 5000
batch_size: 1500
mbpo_rollout_length: 0